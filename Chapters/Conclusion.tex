\chapter{Conclusion and Future Work}\label{chap:conclusion-future-work}

In this dissertation, we investigated three key research questions concerning the transferability and interpretability of machine learning models for network intrusion detection:

\begin{enumerate}
    \item How effectively do machine learning models trained on one dataset transfer and generalise to another in the context of network intrusion detection?
    \item What impact do dataset biases and representativeness have on the performance of machine learning-based intrusion detection systems?
    \item How can explainable AI techniques, such as SHAP, provide insights into the transferability of learned patterns and identify the most relevant features for detecting specific attacks across different datasets?
\end{enumerate}

To address these questions, we conducted experiments using the CTU13 and CICIDS2017 datasets. We trained Random Forest and Support Vector Machine (SVM) classifiers on one dataset and evaluated their performance and transferability on the other. Additionally, we applied the SHAP (SHapley Additive exPlanations) technique to interpret the modelsâ€™ predictions and identify critical features for detecting malicious traffic.

\section{Key Findings}

Our findings reveal significant challenges in transferring machine learning models across network intrusion datasets:

\begin{itemize}
    \item \textbf{Random Forest Performance}: The Random Forest classifiers demonstrated strong performance when trained and tested on the same dataset, achieving accuracy, precision, recall, and F1 scores above 0.99 for binary classification on both CTU13 and CICIDS2017. However, their effectiveness deteriorated markedly when applied to a different dataset. For instance, the Random Forest model trained on CICIDS2017 and tested on CTU13 experienced a drop in accuracy to 0.58, precision to 0.65, recall to 0.02, and F1 score to 0.04.
    \item \textbf{SVM Challenges}: Similarly, the SVM classifiers struggled with transferability and failed to learn effective decision boundaries even on their training datasets. These results underscore the profound impact of dataset biases and the necessity of representative training data for developing robust intrusion detection models.
    \item \textbf{SHAP Analysis Insights}: The SHAP analysis offered valuable insights into the factors underlying these transferability challenges. We observed substantial shifts in feature importance when models were applied to different datasets, indicating that the discriminative power of specific attributes varies significantly with data distribution. This variability complicates maintaining model effectiveness in new environments.
\end{itemize}

\section{Limitations}

A limitation of this research was the inability to compute SHAP values for the SVM classifiers due to the high computational demands of the kernelExplainer. The standard SHAP library on CPU faced performance and memory issues, while the GPU-accelerated version was constrained by insufficient dedicated GPU memory. This limitation highlights the computational challenges of applying explainable AI techniques to certain models.

\section{Future Research Directions}

To address the transferability challenges identified, future research could explore several avenues:

\begin{itemize}
    \item \textbf{Transfer Learning and Domain Adaptation}: Techniques such as fine-tuning models on labelled subsets of target data or unsupervised domain adaptation could help adapt learned patterns to new environments by aligning feature distributions between source and target datasets.
    \item \textbf{Robust Feature Engineering}: Developing methods to identify dataset-invariant representations may enhance model transferability by focusing on features that remain discriminative across diverse datasets.
    \item \textbf{Advanced Explainable AI Techniques}: While SHAP provided insights into feature importance, future work could develop explainability methods tailored to cybersecurity, capturing complex interactions and temporal dependencies in network traffic. Granular, contextualised explanations could further improve the interpretability and trustworthiness of intrusion detection systems.
    \item \textbf{Multi-Source Data Integration}: Combining network traffic data with host-based logs, application-level events, and external threat intelligence could yield more comprehensive and resilient detection models, providing a holistic view of the cybersecurity landscape.
\end{itemize}

\section{Conclusion}

In conclusion, this dissertation highlights the critical importance of evaluating model transferability and interpretability in network intrusion detection. Our experiments demonstrate the difficulties of applying machine learning models trained on one dataset to another, emphasising the need for representative training data and techniques to mitigate dataset biases. The SHAP analysis elucidates the factors contributing to these challenges, such as shifts in feature importance and potential overfitting to dataset-specific patterns.

By advancing our understanding of these issues and fostering the development of more transferable and interpretable models, we can progress towards practical, reliable machine learning-based network intrusion detection systems. The results and insights presented here lay a foundation for future research in this vital area, contributing to ongoing efforts to protect digital infrastructure from evolving cyber threats.