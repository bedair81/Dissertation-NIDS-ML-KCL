\chapter{Specification \& Design}\label{chap:specification-design}

\tikzstyle{dataset} = [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
\tikzstyle{decision} = [rectangle, rounded corners, text centered, draw=black, fill=green!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

This chapter outlines the experiments conducted in this dissertation, focusing on three classifiers: the Dummy Classifier, the Random Forest Classifier, and the Support Vector Machine Classifier. The experimental setup has been designed based on a thorough literature review to address the following research questions:

\begin{enumerate}
\item[\textbf{RQ1}] How well do machine learning models trained on one dataset transfer and generalise to another dataset in the context of network intrusion detection?
\item[\textbf{RQ2}] What is the impact of dataset biases and representativeness on the performance of machine learning-based intrusion detection systems?
\item[\textbf{RQ3}] How can explainable AI techniques, such as SHAP, provide insights into the transferability of learned patterns and the most relevant features for detecting specific types of attacks across different datasets?
\end{enumerate}

The CTU13 and CICIDS2017 datasets are chosen for this dissertation as they contain specific types of attacks and possess distinct properties that make them suitable for addressing the research questions. CTU13 consists of real botnet traffic captured in a controlled environment~\cite{garcia2014empirical}, while CICIDS2017 is a more recent and comprehensive dataset designed for evaluating network intrusion detection systems, containing a wide range of modern attacks~\cite{sharafaldin2018toward}. By training models on CICIDS2017 and testing them on CTU13, this dissertation aims to assess the transferability of learned patterns and features from one dataset to another, addressing RQ1.

The scripts~\ref{alg:relabelCICIDS2017} and~\ref{alg:relabelCTU13} are utilised during pre-processing to ensure uniformity and coherence across all datasets. These scripts facilitate mapping dataset features to a standardised naming convention and removing features that only appear in one dataset (which would not be transferrable between datasets), promoting equitable comparisons and analysis. This approach effectively addresses RQ2 by mitigating the impact of dataset biases and representativeness.

This dissertation comprehensively analyses network intrusion detection classifiers, utilising a range of evaluation metrics, including accuracy, recall, precision, the confusion matrix, and F1 score. Additionally, the SHAP library interprets the classifiers' predictions, offering valuable insights into their decision-making process. An essential aspect of this dissertation is the exploration of RQ3, where we utilise explainable AI techniques to uncover the transferability of learned patterns and identify critical features for detecting specific types of attacks across multiple datasets.

A common practice in machine learning is to split data into training and testing sets using a 60/40 ratio. This approach ensures a fair data distribution between the two sets, which helps prevent overfitting and promotes unbiased testing. This partitioning ratio has been widely accepted in the field as it balances training and assessing models, as noted in a research survey by Buczak et al.~\cite{buczak2015survey}.

Alternative designs and dataset selections were considered, such as using neural network-based classifiers (e.g., MLP and CNN) and other datasets. However, the focus on interpretability using SHAP and the comparative study by Belouch et al.~\cite{belouch2018performance} influenced the decision to use Random Forest and SVM classifiers. The combination of CICIDS2017 and CTU13 datasets aligns with the research objectives of assessing the effectiveness of machine learning classifiers in detecting botnet attacks and understanding the challenges and limitations of transferring learned patterns across datasets.

\section{Experiments}\label{sec:Experiments}

The experiments we run consist of three classifiers: a Dummy Classifier, a Random Forest Classifier, and a Support Vector Machine Classifier. Each classifier is trained on the CICIDS2017 dataset and tested on both the CICIDS2017 and CTU13 datasets to assess its transferability and generalisation capabilities.

\subsection{Data Pre-processing}\label{subsec:DataPreprocessing}

During the pre-processing stage, the datasets undergo relabeling using the scripts~\ref{alg:relabelCICIDS2017} and~\ref{alg:relabelCTU13} to ensure consistency and compatibility. Subsequently, the CICIDS2017 dataset is processed to generate binary and multi-class classification tasks. In contrast, we only process the CTU13 dataset to generate binary classification tasks, aiming to detect botnet attacks and benign traffic because it only includes botnet attacks, whereas CICIDS2017 contains various types of attacks.

\subsection{Training and Testing Split}\label{subsec:TrainingTestingSplit}

We partitioned the datasets into training and testing sets to ensure precise results with a 60/40 ratio. Our team trained the classifiers using the training set from the CICIDS2017 dataset and subsequently tested them on both the testing set of the same dataset and the full CTU13 dataset. This approach allows us to evaluate the classifiers' effectiveness on the dataset we trained them on and their performance on a separate dataset.

\subsection{Evaluation Metrics}\label{subsec:EvaluationMetrics}

Several metrics evaluate the classifiers' performance, including confusion matrix, accuracy, precision, recall, and F1 score. These metrics comprehensively assess the classifiers' effectiveness in detecting network intrusions and their ability to transfer learned patterns across datasets.

\subsection{Explainable AI}\label{subsec:ExplainableAI}

The SHAP library is utilised to interpret the predictions made by the trained classifiers and offer valuable insights into their decision-making process. We compute SHAP values for the Random Forest and Support Vector Machine classifiers to pinpoint the most significant features that aid in detecting particular types of attacks. This aspect of explainability is critical in comprehending the transferability of learned patterns and the essential attributes that differentiate between malicious and benign traffic, even across diverse datasets.

\section{Classifier Configurations}\label{sec:ClassifierConfigurations}

\subsection{Dummy Classifier}\label{subsec:DummyClassifier}

\textbf{Purpose:} The Dummy Classifier serves as a baseline for evaluating the performance of the more advanced classifiers. It randomly assigns labels to the data, providing a reference point for comparison.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1.5cm]
    \node (dataset1) [dataset] {CICIDS2017 Dataset};
    \node (pre-processing) [process, below=of dataset1] {Pre-processing};
    \node (dummyclassifier) [process, below=of pre-processing] {Dummy Classifier};
    \node (trainedmodel) [process, below=of dummyclassifier] {Trained Classifier};
    \node (evaluation1) [decision, below left=0.2cm and -0.5cm of trainedmodel] {Evaluation on CICIDS2017 Testing Set};
    \node (evaluation2) [decision, below right=0.2cm and -0.5cm of trainedmodel] {Evaluation on CTU13 Dataset};
    
    \draw [arrow] (dataset1) -- (pre-processing);
    \draw [arrow] (pre-processing) -- node[anchor=east] {Training Data (60\%)} (dummyclassifier);
    \draw [arrow] (dummyclassifier) -- (trainedmodel);
    \draw [arrow] (trainedmodel) -| (evaluation1);
    \draw [arrow] (trainedmodel) -| (evaluation2);
    \end{tikzpicture}
    \caption{Dummy Classifier Configuration}\label{fig:DummyClassifierConfig}
\end{figure}

\subsection{Random Forest and SVM Classifiers}\label{subsec:RandomForest+SVMClassifier}

\textbf{Purpose:} The Random Forest and Support Vector Machine (SVM) classifiers are employed to evaluate the performance of more advanced machine learning models in detecting network intrusions. These classifiers are trained on the CICIDS2017 dataset and tested on both the CICIDS2017 testing set and the CTU13 dataset to assess their transferability and generalisation capabilities.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1.5cm]
    \node (dataset1) [dataset] {CICIDS2017 Dataset};
    \node (pre-processing) [process, below=of dataset1] {Pre-processing};
    \node (classifier) [process, below=of pre-processing] {Random Forest/SVM Classifier};
    \node (trainedmodel) [process, below=of classifier] {Trained Classifier};
    \node (shap) [process, below=of trainedmodel] {SHAP};
    \node (evaluation1) [decision, below left=0.2cm and -0.5cm of shap] {Evaluation on CICIDS2017 Testing Set};
    \node (evaluation2) [decision, below right=0.2cm and -0.5cm of shap] {Evaluation on CTU13 Dataset};
    
    \draw [arrow] (dataset1) -- (pre-processing);
    \draw [arrow] (pre-processing) -- node[anchor=east] {Training Data (60\%)} (classifier);
    \draw [arrow] (classifier) -- (trainedmodel);
    \draw [arrow] (trainedmodel) -- (shap);
    \draw [arrow] (shap) -| (evaluation1);
    \draw [arrow] (shap) -| (evaluation2);
    \end{tikzpicture}
    \caption{Random Forest and SVM Classifier Configuration}\label{fig:RandomForest+SVMClassifierClassifierConfig}
\end{figure}

\section{Transferability Evaluation}\label{sec:TransferabilityEvaluation}

In evaluating the classifiers' transferability from the CICIDS2017 dataset to the CTU13 dataset, we emphasised the significance of measuring their capability to generalise the learned patterns. To assess this capacity, the classifiers were rigorously trained on the CICIDS2017 dataset and subsequently tested on both its testing set and the entirety of the CTU13 dataset.

To effectively evaluate the transferability of our classifiers, we utilised the following performance metrics, accompanied by their respective mathematical expressions:

\begin{itemize}
    \item \textbf{Accuracy}. Accuracy quantifies the proportion of true predictions, encompassing both positive and negative outcomes, in relation to the total number of observations. It is calculated as:
    \begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
    \end{equation}
    where $TP$, $TN$, $FP$, and $FN$ represent the counts of true positives, true negatives, false positives, and false negatives, respectively.
    
    \item \textbf{Recall}. Recall, or the True Positive Rate, measures the proportion of actual positive cases correctly identified by the model, given by:
    \begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
    
    \item \textbf{Precision}. Precision assesses the fraction of correctly predicted positive observations out of all predicted positives, computed as:
    \begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
    
    \item \textbf{F1 Score}. The F1 score integrates precision and recall into a single metric by taking their harmonic mean, thus providing a balanced measure of the classifier's precision and recall, calculated as:
    \begin{equation}
    F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
\end{itemize}

Calculating these metrics for each classifier on both the CICIDS2017 testing set and the CTU13 dataset enables evaluation of the transferability of the learned patterns. A classifier that maintains high-performance metrics on both datasets demonstrates good transferability, whereas a significant drop in performance on the CTU13 dataset indicates limited transferability.

Furthermore, the SHAP values generated through the explainable AI analysis provide valuable insights into the essential attributes that facilitate the identification of distinct attack types across a range of datasets. By carefully examining the consistency of the most significant features across these datasets, we can assess the applicability of the acquired patterns. The SHAP values obtained through the explainable AI analysis also offer crucial information on the features that contribute the most to detecting particular attack types across various datasets. By thoroughly examining the top-ranked features' consistency across datasets, we can further evaluate the transferability of the learned patterns.

\section{Summary}\label{sec:Summary}

This chapter presents the specification and design of the experiments conducted in this dissertation, focusing on three classifiers: a Dummy Classifier, a Random Forest Classifier, and a Support Vector Machine Classifier. The experimental setup addresses three key research questions related to the transferability and generalisation of machine learning models, the impact of dataset biases, and the insights provided by explainable AI techniques in the context of network intrusion detection.

We chose the CTU13 and CICIDS2017 datasets for their distinct properties and the presence of specific types of attacks. The data pre-processing step involves relabeling the datasets to ensure consistency and compatibility, while the training and testing split follows a 60/40 ratio. The classifiers are trained on the CICIDS2017 dataset and tested on both the CICIDS2017 testing set and the entire CTU13 dataset to evaluate their transferability.

We assess the classifiers' performance using various evaluation metrics, including accuracy, precision, recall, and F1 score. The SHAP library is employed to interpret the predictions of the trained classifiers and identify the most relevant features contributing to the detection of specific types of attacks across different datasets.

The evaluation of transferability is of immense importance to this dissertation. We utilise quality metrics and SHAP values to assess the classifiers' proficiency in transferring learned patterns from CICIDS2017 to CTU13. By analysing the consistency of the top-ranking features and comparing the classifiers' performance on both datasets, we can gauge the transferability of the learned patterns.