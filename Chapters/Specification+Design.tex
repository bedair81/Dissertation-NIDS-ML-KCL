\chapter{Specification \& Design}

This chapter outlines the experiments conducted in this dissertation, focusing on three classifiers: the Dummy Classifier, the Random Forest Classifier, and the Support Vector Machine Classifier. The experimental setup has been designed based on a thorough literature review to address the following research questions:

\begin{enumerate}
\item[\textbf{RQ1}] How well do machine learning models trained on one dataset transfer and generalise to another dataset in the context of network intrusion detection?
\item[\textbf{RQ2}] What is the impact of dataset biases and representativeness on the performance of machine learning-based intrusion detection systems?
\item[\textbf{RQ3}] How can explainable AI techniques, such as SHAP, provide insights into the transferability of learned patterns and the most relevant features for detecting specific types of attacks across different datasets?
\end{enumerate}

The CTU13 and CICIDS2017 datasets are chosen for this dissertation as they contain specific types of attacks and possess distinct properties that make them suitable for addressing the research questions. CTU13 consists of real botnet traffic captured in a controlled environment~\cite{garcia2014empirical}, while CICIDS2017 is a more recent and comprehensive dataset designed for evaluating network intrusion detection systems, containing a wide range of modern attacks~\cite{sharafaldin2018toward}. By training models on CICIDS2017 and testing them on CTU13, this dissertation aims to assess the transferability of learned patterns and features from one dataset to another, addressing RQ1.

The relabelCTU13.py and relabelCICIDS2017.py scripts are utilised during pre-processing to ensure uniformity and coherence across all datasets. These scripts facilitate mapping dataset features to a standardised naming convention and removing features that only appear in one dataset (which would not be transferrable between datasets), promoting equitable comparisons and analysis. This approach effectively addresses RQ2 by mitigating the impact of dataset biases and representativeness.

This dissertation comprehensively analyses network intrusion detection classifiers, utilising a range of evaluation metrics, including accuracy, recall, precision, the confusion matrix, and F1 score. Additionally, the SHAP library interprets the classifiers' predictions, offering valuable insights into their decision-making process. An essential aspect of this dissertation is the exploration of RQ3, where we utilise explainable AI techniques to uncover the transferability of learned patterns and identify critical features for detecting specific types of attacks across multiple datasets.

A common practice in machine learning is to split data into training and testing sets using a 60/40 ratio. This approach ensures a fair data distribution between the two sets, which helps prevent overfitting and promotes unbiased testing. This partitioning ratio has been widely accepted in the field as it balances training and assessing models, as noted in a research survey by Buczak et al.~\cite{buczak2015survey}.

Alternative designs and dataset selections were considered, such as using neural network-based classifiers (e.g., MLP and CNN) and other datasets. However, the focus on interpretability using SHAP and the comparative study by Belouch et al.~\cite{belouch2018performance} influenced the decision to use Random Forest and SVM classifiers. The combination of CICIDS2017 and CTU13 datasets aligns with the research objectives of assessing the effectiveness of machine learning classifiers in detecting botnet attacks and understanding the challenges and limitations of transferring learned patterns across datasets.

\section{Experimental Setup}\label{sec:ExperimentalSetup}

The experimental setup consists of three classifiers: a Dummy Classifier, a Random Forest Classifier, and a Support Vector Machine Classifier. Each classifier is trained on the CICIDS2017 dataset and tested on both the CICIDS2017 and CTU13 datasets to assess its transferability and generalisation capabilities.

\subsection{Data Pre-processing}\label{subsec:DataPreprocessing}

During the pre-processing stage, the datasets undergo relabeling using the relabelCTU13.py and relabelCICIDS2017.py scripts to ensure consistency and compatibility. Subsequently, the CICIDS2017 dataset is processed to generate binary and multi-class classification tasks. In contrast, we only process the CTU13 dataset to generate binary classification tasks, aiming to detect botnet attacks and benign traffic because it only includes botnet attacks, whereas CICIDS2017 contains various types of attacks.

\subsection{Training and Testing Split}\label{subsec:TrainingTestingSplit}

We partitioned the datasets into training and testing sets to ensure precise results with a 60/40 ratio. Our team trained the classifiers using the training set from the CICIDS2017 dataset and subsequently tested them on both the testing set of the same dataset and the full CTU13 dataset. This approach allows us to evaluate the classifiers' effectiveness on the dataset we trained them on and their performance on a separate dataset.

\subsection{Evaluation Metrics}\label{subsec:EvaluationMetrics}

Several metrics evaluate the classifiers' performance, including confusion matrix, accuracy, precision, recall, and F1 score. These metrics comprehensively assess the classifiers' effectiveness in detecting network intrusions and their ability to transfer learned patterns across datasets.

\subsection{Explainable AI}\label{subsec:ExplainableAI}

The SHAP library is utilised to interpret the predictions made by the trained classifiers and offer valuable insights into their decision-making process. We compute SHAP values for the Random Forest and Support Vector Machine classifiers to pinpoint the most significant features that aid in detecting particular types of attacks. This aspect of explainability is critical in comprehending the transferability of learned patterns and the essential attributes that differentiate between malicious and benign traffic, even across diverse datasets.

\section{Classifier Configurations}\label{sec:ClassifierConfigurations}

\subsection{Dummy Classifier}\label{subsec:DummyClassifier}

\textbf{Purpose:} The Dummy Classifier serves as a baseline for evaluating the performance of the more advanced classifiers. It randomly assigns labels to the data, providing a reference point for comparison.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1.5cm]
    \node (dataset1) [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black] {CICIDS2017 Dataset};
    \node (pre-processing) [rectangle, draw, below=of dataset1] {Pre-processing};
    \node (dummyclassifier) [rectangle, draw, below=of pre-processing] {Dummy Classifier};
    \node (trainedmodel) [rectangle, draw, below=of dummyclassifier] {Trained Classifier};
    \node (evaluation1) [rectangle, rounded corners, text centered, draw=black, below left=0.2cm of trainedmodel] {Evaluation on CICIDS2017 Testing Set};
    \node (evaluation2) [rectangle, rounded corners, text centered, draw=black, below right=0.2cm of trainedmodel] {Evaluation on CTU13 Dataset};
    
    \draw [->] (dataset1) -- (pre-processing);
    \draw [->] (pre-processing) -- node[anchor=east] {Training Data (60\%)} (dummyclassifier);
    \draw [->] (dummyclassifier) -- (trainedmodel);
    \draw [->] (trainedmodel) -| (evaluation1);
    \draw [->] (trainedmodel) -| (evaluation2);
    \end{tikzpicture}
    \caption{Dummy Classifier Configuration}\label{fig:DummyClassifierConfig}
\end{figure}

\subsection{Random Forest and SVM Classifiers}\label{subsec:RandomForest+SVMClassifier}

\textbf{Purpose:} The Random Forest and Support Vector Machine (SVM) classifiers are employed to evaluate the performance of more advanced machine learning models in detecting network intrusions. These classifiers are trained on the CICIDS2017 dataset and tested on both the CICIDS2017 testing set and the CTU13 dataset to assess their transferability and generalisation capabilities.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.5cm]
\node (dataset1) [trapezium, trapezium left angle=70, trapezium right angle=110, text centered, draw=black] {CICIDS2017 Dataset};
\node (pre-processing) [rectangle, draw, below=of dataset1] {Pre-processing};
\node (classifier) [rectangle, draw, below=of pre-processing] {Random Forest/SVM Classifier};
\node (trainedmodel) [rectangle, draw, below=of classifier] {Trained Classifier};
\node (shap) [rectangle, draw, below=of trainedmodel] {SHAP};
\node (evaluation1) [rectangle, rounded corners, text centered, draw=black, below left=0.2cm of shap] {Evaluation on CICIDS2017 Testing Set};
\node (evaluation2) [rectangle, rounded corners, text centered, draw=black, below right=0.2cm of shap] {Evaluation on CTU13 Dataset};

\draw [->] (dataset1) -- (pre-processing);
\draw [->] (pre-processing) -- node[anchor=east] {Training Data (60\%)} (classifier);
\draw [->] (classifier) -- (trainedmodel);
\draw [->] (trainedmodel) -- (shap);
\draw [->] (shap) -| (evaluation1);
\draw [->] (shap) -| (evaluation2);
\end{tikzpicture}
\caption{Random Forest Classifier Configuration}\label{fig:RandomForest+SVMClassifierClassifierConfig}
\end{figure}

\section{Transferability Evaluation}\label{sec:TransferabilityEvaluation}

To ensure the applicability of our findings, we took great care in constructing the experimental setup for our dissertation. Our goal was to assess the classifiers' ability to apply the learned patterns from the CICIDS2017 dataset to the CTU13 dataset. We recognise the significance of transferability and thus designed the setup to test the classifiers' capability to transfer their learned patterns. During the training process, the classifiers were trained on the CICIDS2017 dataset and assessed on its testing set and the entire CTU13 dataset.

In order to assess the transferability of the classifiers, we consider the following performance metrics:

\begin{itemize}
\item Accuracy: The overall correctness of the classifier's predictions on the target dataset.
\item Precision: The proportion of true positive predictions among all positive predictions.
\item Recall: The proportion of true positive predictions among all real positive instances.
\item F1 score: The harmonic mean of precision and recall, providing a balanced measure of the classifier's performance.
\end{itemize}

Calculating these metrics for each classifier on both the CICIDS2017 testing set and the CTU13 dataset enables evaluation of the transferability of the learned patterns. A classifier that maintains high-performance metrics on both datasets demonstrates good transferability, whereas a significant drop in performance on the CTU13 dataset indicates limited transferability.

Furthermore, the SHAP values generated through the explainable AI analysis provide valuable insights into the essential attributes that facilitate the identification of distinct attack types across a range of datasets. By carefully examining the consistency of the most significant features across these datasets, we can assess the applicability of the acquired patterns. The SHAP values obtained through the explainable AI analysis also offer crucial information on the features that contribute the most to detecting particular attack types across various datasets. By thoroughly examining the top-ranked features' consistency across datasets, we can further evaluate the transferability of the learned patterns.

\section{Summary}\label{sec:Summary}

This chapter presents the specification and design of the experiments conducted in this dissertation, focusing on three classifiers: a Dummy Classifier, a Random Forest Classifier, and a Support Vector Machine Classifier. The experimental setup addresses three key research questions related to the transferability and generalisation of machine learning models, the impact of dataset biases, and the insights provided by explainable AI techniques in the context of network intrusion detection.

We chose the CTU13 and CICIDS2017 datasets for their distinct properties and the presence of specific types of attacks. The data pre-processing step involves relabeling the datasets to ensure consistency and compatibility, while the training and testing split follows a 60/40 ratio. The classifiers are trained on the CICIDS2017 dataset and tested on both the CICIDS2017 testing set and the entire CTU13 dataset to evaluate their transferability.

We assess the classifiers' performance using various evaluation metrics, including accuracy, precision, recall, and F1 score. The SHAP library is employed to interpret the predictions of the trained classifiers and identify the most relevant features contributing to the detection of specific types of attacks across different datasets.

The evaluation of transferability is of immense importance to this dissertation. We utilise quality metrics and SHAP values to assess the classifiers' proficiency in transferring learned patterns from CICIDS2017 to CTU13. By analysing the consistency of the top-ranking features and comparing the classifiers' performance on both datasets, we can gauge the transferability of the learned patterns.