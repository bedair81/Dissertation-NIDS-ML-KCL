\chapter{Implementation}
This chapter provides a comprehensive overview of the experiments conducted in this dissertation, detailing the experimental setup, dataset usage, and the design of each experiment. We also explore the implementation of key package modules, accompanied by relevant code excerpts highlighting essential functionality.

\section{Experimental Setup}\label{sec:experimental-setup}
The experiments in this dissertation involve training and evaluating three classifiers: a Dummy Classifier, a Random Forest Classifier, and a Support Vector Machine (SVM) Classifier. Each classifier is trained on the CICIDS2017 dataset~\cite{sharafaldin2018toward} and tested on both the CICIDS2017 and CTU13~\cite{garcia2014empirical} datasets to assess transferability and generalization capabilities.

The choice of Random Forest and SVM classifiers is justified by their proven effectiveness in handling large, high-dimensional datasets and unbalanced class distributions~\cite{farnaaz2016random, teng2017svm}, which are common in network traffic classification. We employ the SHAP library~\cite{lundberg2017unified} for explainability due to its ability to provide insights into the decision-making process of machine learning models and identify important features.

\subsection{Data Pre-processing}\label{subsec:pre-processing}
Proper data preprocessing is crucial for ensuring the effectiveness of the trained classifiers. The relabelling scripts, \texttt{relabelCICIDS2017.py} (Section~\ref{subsec:relabelCICIDS2017.py}) and \texttt{relabelCTU13.py} (Section~\ref{subsec:relabelCTU13.py}), standardize feature names and class labels across the datasets. This process involves mapping dataset features to a consistent naming convention, removing features unique to one dataset to avoid overfitting, and ensuring class labels are compatible. For example, the CTU13 dataset's binary labels '0' and '1' are converted to `Benign' and `Botnet' to match the CICIDS2017 labelling scheme.

During the preprocessing stage, several data quality issues and inconsistencies were encountered, such as missing values and differing feature names across datasets. We address these issues through careful data cleaning, imputation, and feature mapping techniques to ensure the datasets' compatibility and integrity.

\subsection{Experiment 1: Dummy Classifier}\label{subsec:baseline-performance}
The Dummy Classifier from the scikit-learn library~\cite{pedregosa2011scikit} serves as a baseline for evaluating the performance of the more advanced classifiers. It predicts the most frequent class in the training data. Three Dummy Classifiers are trained: one on the CTU13 dataset for binary classification and two on the CICIDS2017 dataset for binary and multiclass classification. The Dummy Classifiers use the same features as the Random Forest and SVM classifiers, and their performance metrics (accuracy, precision, recall, and F1 score) provide a baseline for comparison.

\subsection{Experiment 2: Random Forest Classifier}\label{subsec:random-forest-classifier}
Random Forest, an ensemble learning method constructing multiple decision trees~\cite{hastie2009random}, is well-suited for handling large, high-dimensional datasets and unbalanced class distributions~\cite{farnaaz2016random}, which are common in network traffic classification. The CTU13 and CICIDS2017 datasets exhibit such class imbalances (Tables~\ref{tab:ctu13_breakdown} and~\ref{tab:cicids2017_breakdown}).

Three Random Forest Classifiers are trained (\texttt{trainRandomForest.ipynb}, Section~\ref{subsec:trainRandomForest.ipynb}): one on the CTU13 dataset, and two on the CICIDS2017 dataset for binary and multiclass classification. The classifiers are evaluated on their respective datasets and then tested on the other dataset to assess transferability. The SHapley Additive exPlanations (SHAP) library~\cite{lundberg2017unified} is employed to explain the classifiers' predictions and identify the essential features.

\subsection{Experiment 3: Support Vector Machine Classifier}\label{subsec:support-vector-machine-classifier}
Support Vector Machines (SVMs) effectively handle large, high-dimensional, non-linear data~\cite{cortes1995support, scholkopf2002learning}. They have been successfully applied to network intrusion detection tasks~\cite{kim2003network, teng2017svm}.

The experimental setup for the SVM Classifiers (\texttt{trainSVM.ipynb}, Section~\ref{subsec:trainSVM.ipynb}) mirrors that of the Random Forest Classifiers: we train three SVMs on the CTU13 and CICIDS2017 datasets, evaluated on their respective datasets, and tested on the other dataset. SHAP is used to interpret the SVM predictions and identify important features.

\subsection{Testing and Evaluation}\label{subsec:testing-evaluation}
A comprehensive testing and evaluation strategy is employed to ensure the robustness and reliability of the results. We assess the performance of the classifiers using various metrics, including accuracy, precision, recall, and F1 score. We calculate these metrics for each classifier on their respective test sets and when evaluated on the other dataset for transferability analysis.

\subsection{Novelty and Originality}\label{subsec:novelty-originality}
The novelty and originality of this research lie in the combination of the chosen datasets (CTU13 and CICIDS2017), classifiers (Random Forest and SVM), and the use of SHAP for explainability. This dissertation provides a unique perspective on network intrusion detection by investigating the transferability and generalization of machine learning models across different datasets.

The application of SHAP to interpret the predictions of the trained classifiers and identify the most relevant features for detecting specific types of attacks across datasets is a novel approach. This aspect of the research contributes to a deeper understanding of the transferability of learned patterns and the key characteristics distinguishing malicious and benign traffic.

\subsection{Strengths and Limitations}\label{subsec:strengths-limitations}
The chosen methodology has several strengths. Random Forest and SVM classifiers are well-suited for handling the imbalanced and high-dimensional nature of the CTU13 and CICIDS2017 datasets. These classifiers have demonstrated their effectiveness in network intrusion detection tasks~\cite{farnaaz2016random, teng2017svm}.

However, there are also limitations to consider. The potential impact of dataset biases and the training data's representativeness on the models' transferability is a concern. The computational complexity of the SHAP explanations may also pose challenges when dealing with large-scale datasets.

\section{Package Implementation}\label{sec:package-implementation}

\subsection{relabelCTU13.py}\label{subsec:relabelCTU13.py}

\begin{algorithm}[H]
\caption{Relabeling CTU13 Dataset}\label{alg:relabelCTU13}
\begin{algorithmic}[1]
\Require%
Raw CTU13 dataset
\Ensure%
Preprocessed CTU13 dataset with consistent feature names and class labels
\State%
Import necessary libraries
\State%
Load the raw CTU13 dataset
\State%
Define a dictionary for mapping feature names
\For{each feature in the CTU13 dataset}
    \If{the feature name exists in the mapping dictionary}
        \State%
        Rename the feature using the corresponding value from the dictionary
    \EndIf%
\EndFor%
\State%
Replace the class labels '0' and '1' with `Benign' and `Botnet', respectively 
\State%
Define the desired order of features based on the CICIDS2017 dataset
\State%
Reorder the features in the CTU13 dataset according to the desired order
\State%
Save the preprocessed CTU13 dataset
\end{algorithmic}
\end{algorithm}

Like \texttt{relabelCICIDS2017.py} (Section~\ref{subsec:relabelCICIDS2017.py}), this script preprocesses the CTU13 dataset by renaming features, relabeling traffic classes, and reordering features to match the CICIDS2017 dataset's structure.

\subsection{relabelCICIDS2017.py}\label{subsec:relabelCICIDS2017.py}

\begin{algorithm}[H]
\caption{Relabeling CICIDS2017 Dataset}\label{alg:relabelCICIDS2017}
\begin{algorithmic}[1]
\Require%
Raw CICIDS2017 and preprocessed CTU13 datasets
\Ensure%
Preprocessed CICIDS2017 dataset with consistent feature names and class labels
\State%
Import necessary libraries
\State%
Load the raw CICIDS2017 dataset
\State%
Load the preprocessed CTU13 dataset
\State%
Define a dictionary for mapping feature names from CTU13 to CICIDS2017
\For{each feature in the CTU13 dataset}
    \If{the feature name exists in the CICIDS2017 dataset}
        \State%
        Rename the feature in the CTU13 dataset using the corresponding value from the mapping dictionary
    \EndIf%
\EndFor%
\State%
Change the label of benign traffic from '0' to `Benign'
\State%
Change the label of attack traffic from '1' to `Botnet'
\State%
Get the list of columns in the preprocessed CTU13 dataset
\State%
Get the common columns between the preprocessed CTU13 and raw CICIDS2017 datasets
\State%
Reorder and select the common columns in the CICIDS2017 dataset
\State%
Save the preprocessed CICIDS2017 dataset
\end{algorithmic}
\end{algorithm}

This script preprocesses the CICIDS2017 dataset by mapping feature names to match the CTU13 dataset, relabeling traffic classes, identifying common features, and reordering the CICIDS2017 features to align with the CTU13 dataset.

\subsection{trainDummyClassifier.ipynb}\label{subsec:trainDummyClassifier.ipynb}

\begin{algorithm}[H]
\caption{Training Dummy Classifiers}\label{alg:trainDummyClassifier}
\begin{algorithmic}[1]
\Require%
Preprocessed CTU13 and CICIDS2017 datasets
\Ensure%
Trained Dummy Classifiers and performance metrics
\State%
Import necessary libraries  
\State%
Read preprocessed CTU13 and CICIDS2017 datasets
\State%
Define common features for training and testing
\For{dataset in [CTU13, CICIDS2017]}
    \If{dataset is CTU13}
        \State%
        Train Dummy Classifier on CTU13 dataset (binary classification)
        \State%
        Save the trained classifier
    \ElsIf{dataset is CICIDS2017}
        \State%
        Train Dummy Classifier on CICIDS2017 dataset (binary classification)
        \State%
        Train Dummy Classifier on CICIDS2017 dataset (multiclass classification)
        \State%
        Save the trained classifiers
    \EndIf%
\EndFor%
\For{each trained Dummy Classifier}
    \State%
    Load the trained classifier
    \State%
    Test the classifier on its corresponding test set as well as the other dataset
    \State%
    Calculate performance metrics (accuracy, precision, recall, F1 score) for each experiment
    \State%
    Save performance metrics for analysis and comparison
\EndFor%
\end{algorithmic}
\end{algorithm}

This Jupyter Notebook trains and evaluates the Dummy Classifiers on the CTU13 and CICIDS2017 datasets. The classifiers' performance is a baseline for comparing the Random Forest and SVM Classifiers.

\subsection{trainRandomForest.ipynb}\label{subsec:trainRandomForest.ipynb}

\begin{algorithm}[H]
\caption{Training Random Forest Classifiers}\label{alg:trainRandomForest}
\begin{algorithmic}[1]
\Require%
Preprocessed CTU13 and CICIDS2017 datasets
\Ensure%
Trained Random Forest Classifiers, performance metrics, and SHAP values
\State%
Import necessary libraries
\State%
Read preprocessed CTU13 and CICIDS2017 datasets
\State%
Define common features for training and testing
\For{dataset in [CTU13, CICIDS2017]}
    \If{dataset is CTU13}
        \State%
        Train Dummy Classifier on CTU13 dataset (binary classification)
        \State%
        Save the trained classifier
    \ElsIf{dataset is CICIDS2017}
        \State%
        Train Dummy Classifier on CICIDS2017 dataset (binary classification)
        \State%
        Train Dummy Classifier on CICIDS2017 dataset (multiclass classification)
        \State%
        Save the trained classifiers
    \EndIf%
\EndFor%
\For{each trained Random Forest Classifier}
    \State%
    Load the trained classifier
    \State%
    Test the classifier on its corresponding test set as well as the other dataset
    \State%
    Calculate performance metrics (accuracy, precision, recall, F1 score) for each experiment
    \State%
    Save performance metrics for analysis and comparison
    \State%
    Create a SHAP explainer object for the classifier
    \State%
    Calculate SHAP values for the test set
    \State%
    Save SHAP values for analysis and comparison
\EndFor%
\end{algorithmic}
\end{algorithm}

This Jupyter Notebook trains and evaluates the Random Forest Classifiers on the CTU13 and CICIDS2017 datasets. It tests the classifiers' transferability by evaluating their performance on the dataset for which they were not trained. The notebook also utilizes the SHAP library to explain the classifiers' predictions and identify important features.

\subsection{trainSVM.ipynb}\label{subsec:trainSVM.ipynb}

\begin{algorithm}[H]
\caption{Training Support Vector Machine Classifiers}\label{alg:trainSVM} 
\begin{algorithmic}[1]
\Require%
Preprocessed CTU13 and CICIDS2017 datasets
\Ensure%
Trained SVM Classifiers, performance metrics, and SHAP values  
\State%
Import necessary libraries
\State%
Read preprocessed CTU13 and CICIDS2017 datasets
\State%
Define common features for training and testing
\For{dataset in [CTU13, CICIDS2017]}
    \If{dataset is CTU13}
        \State%
        Train Dummy Classifier on CTU13 dataset (binary classification)
        \State%
        Save the trained classifier
    \ElsIf{dataset is CICIDS2017}
        \State%
        Train Dummy Classifier on CICIDS2017 dataset (binary classification)
        \State%
        Train Dummy Classifier on CICIDS2017 dataset (multiclass classification)
        \State%
        Save the trained classifiers
    \EndIf%
\EndFor%
\For{each trained SVM Classifier}
    \State%
    Load the trained classifier
    \State%
    Test the classifier on its corresponding test set as well as the other dataset
    \State%
    Calculate performance metrics (accuracy, precision, recall, F1 score) for each experiment
    \State%
    Save performance metrics for analysis and comparison
    \State%
    Create a SHAP explainer object for the classifier
    \State%
    Calculate SHAP values for the test set
    \State%
    Save SHAP values for analysis and comparison
\EndFor%
\end{algorithmic}
\end{algorithm}

Similar to \texttt{trainRandomForest.ipynb} (Section~\ref{subsec:trainRandomForest.ipynb}), this Jupyter Notebook trains and evaluates the SVM Classifiers on the CTU13 and CICIDS2017 datasets, tests their transferability, and employs SHAP to interpret the classifiers' predictions and identify important features.
