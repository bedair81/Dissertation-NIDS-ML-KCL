\chapter{Implementation}\label{chap:implementation}

This chapter provides a comprehensive overview of the experiments conducted in this dissertation, detailing the experimental setup, dataset usage, and the design of each experiment. We also explore the implementation of key package modules, accompanied by relevant code excerpts highlighting essential functionality.

\section{Experiment Implementation}\label{sec:ExperimentImplementation}
The experiments in this dissertation involve training and evaluating three classifiers: a Dummy Classifier, a Random Forest Classifier, and a Support Vector Machine (SVM) Classifier. Each classifier is trained on the CICIDS2017 dataset~\cite{sharafaldin2018toward} and tested on both the CICIDS2017 and CTU13~\cite{garcia2014empirical} datasets to assess transferability and generalisation capabilities.

Initially, we set up the experiments using sci-kit-learn's classifier implementations. However, due to the extensive training times required for the CPU-based models, we decided to switch to CUML's GPU-accelerated models~\cite{raschka2020machine}. The CUML library provides GPU-accelerated machine learning algorithms, enabling faster training and inference times. By leveraging the power of GPUs, the training process can be significantly accelerated, making it more feasible to train complex models on large datasets. The benefits of using CUML's GPU-accelerated models include reduced training times, improved scalability, and the ability to handle larger datasets efficiently.

The choice of Random Forest and SVM classifiers is justified by their proven effectiveness in handling large, high-dimensional datasets and unbalanced class distributions~\cite{farnaaz2016random, teng2017svm}, which are common in network traffic classification. We employ the SHAP library~\cite{lundberg2017unified} for explainability due to its ability to provide insights into the decision-making process of machine learning models and identify important features.

\subsection{Data Pre-processing}\label{subsec:pre-processing}
Proper data preprocessing is crucial for ensuring the effectiveness of the trained classifiers. The relabelling scripts, \texttt{relabelCICIDS2017.py} (Section~\ref{alg:relabelCICIDS2017}) and \texttt{relabelCTU13.py} (Section~\ref{alg:relabelCTU13}), standardise feature names and class labels across the datasets. This process involves mapping dataset features to a consistent naming convention, removing features unique to one dataset to avoid overfitting, and ensuring class labels are compatible. For example, the CTU13 dataset's binary labels '0' and '1' are converted to `Benign' and `Botnet' to match the CICIDS2017 labelling scheme.

During the preprocessing stage, several data quality issues and inconsistencies were encountered, such as missing values and differing feature names across datasets. We address these issues through careful data cleaning, imputation, and feature mapping techniques to ensure the datasets' compatibility and integrity.

\subsection{Experiment 1: Dummy Classifier}\label{subsec:baseline-performance}
The Dummy Classifier from the scikit-learn library~\cite{pedregosa2011scikit} serves as a baseline for evaluating the performance of the more advanced classifiers. It predicts the most frequent class in the training data. We train three Dummy Classifiers: one on the CTU13 dataset for binary classification and two on the CICIDS2017 dataset for binary and multiclass classification. The Dummy Classifiers use the same features as the Random Forest and SVM classifiers, and their performance metrics (accuracy, precision, recall, and F1 score) provide a baseline for comparison.

\subsection{Experiment 2: Random Forest Classifier}\label{subsec:random-forest-classifier}
Random Forest, an ensemble learning method constructing multiple decision trees~\cite{hastie2009random}, is well-suited for handling large, high-dimensional datasets and unbalanced class distributions~\cite{farnaaz2016random}, which are common in network traffic classification. The CTU13 and CICIDS2017 datasets exhibit such class imbalances (Tables~\ref{tab:ctu13_breakdown} and~\ref{tab:cicids2017_breakdown}).

Three Random Forest Classifiers are trained (\texttt{trainRandomForest.ipynb}, Section~\ref{alg:trainRandomForest}): one on the CTU13 dataset, and two on the CICIDS2017 dataset for binary and multiclass classification. The classifiers are evaluated on their respective datasets and then tested on the other dataset to assess transferability. The SHapley Additive exPlanations (SHAP) library~\cite{lundberg2017unified} is employed to explain the classifiers' predictions and identify the essential features.

\subsection{Experiment 3: Support Vector Machine Classifier}\label{subsec:support-vector-machine-classifier}
Support Vector Machines (SVMs) effectively handle large, high-dimensional, non-linear data~\cite{cortes1995support, scholkopf2002learning}. They have been successfully applied to network intrusion detection tasks~\cite{kim2003network, teng2017svm}.

The experimental setup for the SVM Classifiers (\texttt{trainSVM.ipynb}, Section~\ref{alg:trainSVM}) mirrors that of the Random Forest Classifiers: we train three SVMs on the CTU13 and CICIDS2017 datasets, evaluated on their respective datasets, and tested on the other dataset. SHAP is used to interpret the SVM predictions and identify important features.

\subsection{Testing and Evaluation}\label{subsec:testing-evaluation}
A comprehensive testing and evaluation strategy is employed to ensure the robustness and reliability of the results. We assess the performance of the classifiers using various metrics, including accuracy, precision, recall, and F1 score. We calculate these metrics for each classifier on their respective test sets and when evaluated on the other dataset for transferability analysis.

\subsection{Novelty and Originality}\label{subsec:novelty-originality}
The novelty and originality of this research lie in the combination of the chosen datasets (CTU13 and CICIDS2017), classifiers (Random Forest and SVM), and the use of SHAP for explainability. This dissertation provides a unique perspective on network intrusion detection by investigating the transferability and generalisation of machine learning models across different datasets.

The application of SHAP to interpret the predictions of the trained classifiers and identify the most relevant features for detecting specific types of attacks across datasets is a novel approach. This aspect of the research contributes to a deeper understanding of the transferability of learned patterns and the key characteristics distinguishing malicious and benign traffic.

\subsection{Strengths and Limitations}\label{subsec:strengths-limitations}
The chosen methodology has several strengths. Random Forest and SVM classifiers are well-suited for handling the imbalanced and high-dimensional nature of the CTU13 and CICIDS2017 datasets. These classifiers have demonstrated their effectiveness in network intrusion detection tasks~\cite{farnaaz2016random, teng2017svm}. Using CUML's GPU-accelerated models significantly reduces training times and improves scalability, enabling the efficient handling of large datasets.

However, there are also limitations to consider. The potential impact of dataset biases and the training data's representativeness on the models' transferability is a concern. The computational complexity of the SHAP explanations may also pose challenges when dealing with large-scale datasets, even with GPU acceleration.

\section{Package Implementation}
This section elucidates the implementation details of the software packages developed as part of this project, focusing on preprocessing scripts for the CTU13 and CICIDS2017 datasets, training modules for various classifiers, and a data visualization tool. Each subsection is dedicated to a specific script or Jupyter Notebook, detailing its purpose, functionalities, and contribution to the overarching project goals.

\subsection{relabelCTU13.py}
\textbf{Purpose}: This Python script~(\ref{alg:relabelCTU13}) is tasked with preprocessing the CTU13 dataset, ensuring its compatibility with the analysis framework. It specifically focuses on feature normalization, class relabeling, and reordering of features to align with the structure of the CICIDS2017 dataset.

\textbf{Functionality}:
\begin{itemize}
    \item \textit{Feature Renaming}: Aligns the CTU13 dataset's feature names with those of CICIDS2017, facilitating direct comparison and joint analysis.
    \item \textit{Traffic Class Relabeling}: Converts traffic classification labels to a unified schema shared with CICIDS2017, enabling consistent interpretation of traffic types across datasets.
    \item \textit{Feature Reordering}: Adjusts the order of features in the CTU13 dataset to match that of CICIDS2017, ensuring that subsequent analysis scripts operate correctly without the need for dataset-specific adjustments.
\end{itemize}

\subsection{relabelCICIDS2017.py}
\textbf{Purpose}: Similar to \texttt{relabelCTU13.py}, this script~(\ref{alg:relabelCICIDS2017}) prepares the CICIDS2017 dataset for analysis by renaming features, relabeling traffic classes, and reordering features. The adjustments ensure that the CICIDS2017 dataset's structure is compatible with CTU13, facilitating combined analyses.

\textbf{Functionality}:
\begin{itemize}
    \item \textit{Mapping Feature Names}: Transforms the feature names in CICIDS2017 to align with CTU13â€™s nomenclature, ensuring consistency in feature interpretation.
    \item \textit{Traffic Class Relabeling and Identification}: Updates the traffic class labels for compatibility and identifies common features between the datasets to focus on comparable data points.
    \item \textit{Feature Reordering}: Modifies the feature order in CICIDS2017 to conform to CTU13's layout, simplifying cross-dataset analyses.
\end{itemize}

\subsection{trainDummyClassifier.ipynb}
\textbf{Purpose}: A Jupyter Notebook~(\ref{alg:trainDummyClassifier}) dedicated to training baseline Dummy Classifiers on the CTU13 and CICIDS2017 datasets. It sets a foundational performance benchmark for comparing more sophisticated Random Forest and SVM classifiers.

\textbf{Functionality}:
\begin{itemize}
    \item \textit{Training Process}: Outlines the steps for training Dummy Classifiers, including data loading, preprocessing application, and classifier training.
    \item \textit{Performance Evaluation}: Details the evaluation metrics used to assess the classifiers, providing a baseline for the effectiveness of subsequent, more complex models.
\end{itemize}

\subsection{trainRandomForest.ipynb}
\textbf{Purpose}: Trains and evaluates Random Forest classifiers~(\ref{alg:trainRandomForest}) on both datasets using GPU-accelerated implementations. It explores the classifiers' transferability and employs the SHAP library for result interpretation.

\textbf{Functionality}:
\begin{itemize}
    \item \textit{GPU-Accelerated Training}: Leverages CUMLâ€™s GPU-accelerated Random Forest implementation for efficient model training and evaluation.
    \item \textit{Transferability Testing}: Assesses the model's performance on both the training dataset and an alternative dataset to examine transferability.
    \item \textit{Feature Importance Analysis}: Utilizes SHAP values to interpret the model's predictions and identify significant features, enhancing model transparency and understanding.
\end{itemize}

\subsection{trainSVM.ipynb}
\textbf{Purpose}: Similar to the Random Forest notebook, this Jupyter Notebook trains SVM classifiers~(\ref{alg:trainSVM}) on the datasets, tests their transferability, and applies SHAP for interpretability.

\textbf{Functionality}:
\begin{itemize}
    \item \textit{GPU-Accelerated SVM Training}: Implements CUMLâ€™s SVM for rapid model training, facilitating the handling of large datasets.
    \item \textit{Cross-Dataset Performance Evaluation}: Evaluates SVM classifiers' performance across different datasets to test model generalizability.
    \item \textit{SHAP-Based Explanation}: Applies SHAP to elucidate SVM classifiers' decision-making, spotlighting critical features that influence predictions.
\end{itemize}

\subsection{plotData.ipynb}
\textbf{Purpose}: This Script~(\ref{alg:plotData}) provides visualizations of dataset characteristics and model performance metrics
