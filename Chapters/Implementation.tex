\chapter{Implementation}

This chapter provides a comprehensive overview of the experiments conducted in this dissertation, detailing the experimental setup, dataset usage, and the design of each experiment. The implementation of key package modules is also explored, accompanied by relevant code excerpts highlighting essential functionality.

\section{Experimental Setup}\label{sec:experimental-setup}

The experiments in this dissertation involve training and evaluating three classifiers: a Dummy Classifier, a Random Forest Classifier, and a Support Vector Machine (SVM) Classifier. Each classifier is trained on the CICIDS2017 dataset~\cite{sharafaldin2018toward} and tested on both the CICIDS2017 and CTU13~\cite{garcia2014empirical} datasets to assess transferability and generalization capabilities.

\subsection{Data Pre-processing}\label{subsec:pre-processing}

Proper data preprocessing is crucial for ensuring the effectiveness of the trained classifiers. The relabelling scripts, \texttt{relabelCICIDS2017.py} (Section~\ref{subsec:relabelCICIDS2017.py}) and \texttt{relabelCTU13.py} (Section~\ref{subsec:relabelCTU13.py}), standardize feature names and class labels across the datasets. This process involves mapping dataset features to a consistent naming convention, removing features unique to one dataset to avoid overfitting, and ensuring class labels are compatible. For example, the CTU13 dataset's binary labels '0' and '1' are converted to `Benign' and `Botnet' to match the CICIDS2017 labelling scheme.

\subsection{Experiment 1: Dummy Classifier}\label{subsec:baseline-performance}

The Dummy Classifier from the scikit-learn library~\cite{pedregosa2011scikit} serves as a baseline for evaluating the performance of the more advanced classifiers. It predicts the most frequent class in the training data. Three Dummy Classifiers are trained: one on the CTU13 dataset for binary classification and two on the CICIDS2017 dataset for binary and multiclass classification. The Dummy Classifiers use the same features as the Random Forest and SVM classifiers, and their performance metrics (accuracy, precision, recall, and F1 score) provide a baseline for comparison.

\subsection{Experiment 2: Random Forest Classifier}\label{subsec:random-forest-classifier}

Random Forest, an ensemble learning method constructing multiple decision trees~\cite{hastie2009random}, is well-suited for handling large, high-dimensional datasets and unbalanced class distributions~\cite{farnaaz2016random}, which are common in network traffic classification. The CTU13 and CICIDS2017 datasets exhibit such class imbalances (Tables~\ref{tab:ctu13_breakdown} and~\ref{tab:cicids2017_breakdown}).

Three Random Forest Classifiers are trained (\texttt{trainRandomForest.ipynb}, Section~\ref{subsec:trainRandomForest.ipynb}): one on the CTU13 dataset, and two on the CICIDS2017 dataset for binary and multiclass classification. The classifiers are evaluated on their respective datasets and then tested on the other dataset to assess transferability. The SHapley Additive exPlanations (SHAP) library~\cite{lundberg2017unified} is employed to explain the classifiers' predictions and identify the essential features.

\subsection{Experiment 3: Support Vector Machine Classifier}\label{subsec:support-vector-machine-classifier}

Support Vector Machines (SVMs) effectively handle large, high-dimensional, non-linear data~\cite{cortes1995support, scholkopf2002learning}. They have been successfully applied to network intrusion detection tasks~\cite{kim2003network, teng2017svm}.

The experimental setup for the SVM Classifiers (\texttt{trainSVM.ipynb}, Section~\ref{subsec:trainSVM.ipynb}) mirrors that of the Random Forest Classifiers: we train three SVMs on the CTU13 and CICIDS2017 datasets, evaluated on their respective datasets, and tested on the other dataset. SHAP is used to interpret the SVM predictions and identify important features.

\section{Package Implementation}\label{sec:package-implementation}

\subsection{relabelCTU13.py}\label{subsec:relabelCTU13.py}

\begin{algorithm}[H]
    \caption{Relabeling CTU13 Dataset}\label{alg:relabelCTU13}
    \begin{algorithmic}[1]
    \Require Raw CTU13 dataset
    \Ensure Preprocessed CTU13 dataset with consistent feature names and class labels
    \State Import necessary libraries
    \State Load the raw CTU13 dataset
    \State Define a dictionary for mapping feature names
    \For{each feature in the CTU13 dataset}
    \If{the feature name exists in the mapping dictionary}
    \State Rename the feature using the corresponding value from the dictionary
    \EndIf
    \EndFor
    \State Replace the class labels '0' and '1' with `Benign' and `Botnet', respectively
    \State Define the desired order of features based on the CICIDS2017 dataset
    \State Reorder the features in the CTU13 dataset according to the desired order
    \State Save the preprocessed CTU13 dataset
    \end{algorithmic}
\end{algorithm}

Like \texttt{relabelCICIDS2017.py}~\ref{subsec:relabelCICIDS2017.py}, this script preprocesses the CTU13 dataset by renaming features, relabeling traffic classes, and reordering features to match the CICIDS2017 dataset's structure.

\subsection{relabelCICIDS2017.py}\label{subsec:relabelCICIDS2017.py}

\begin{algorithm}[H]
    \caption{Relabeling CICIDS2017 Dataset}\label{alg:relabelCICIDS2017}
    \begin{algorithmic}[1]
    \Require Raw CICIDS2017 and preprocessed CTU13 datasets
    \Ensure Preprocessed CICIDS2017 dataset with consistent feature names and class labels
    \State Import necessary libraries
    \State Load the raw CICIDS2017 dataset
    \State Load the preprocessed CTU13 dataset
    \State Define a dictionary for mapping feature names from CTU13 to CICIDS2017
    \For{each feature in the CTU13 dataset}
    \If{the feature name exists in the CICIDS2017 dataset}
    \State Rename the feature in the CTU13 dataset using the corresponding value from the mapping dictionary
    \EndIf
    \EndFor
    \State Change the label of benign traffic from '0' to `Benign'
    \State Change the label of attack traffic from '1' to `Botnet'
    \State Get the list of columns in the preprocessed CTU13 dataset
    \State Get the common columns between the preprocessed CTU13 and raw CICIDS2017 datasets
    \State Reorder and select the common columns in the CICIDS2017 dataset
    \State Save the preprocessed CICIDS2017 dataset
    \end{algorithmic}
\end{algorithm}

This script preprocesses the CICIDS2017 dataset by mapping feature names to match the CTU13 dataset, relabeling traffic classes, identifying common features, and reordering the CICIDS2017 features to align with the CTU13 dataset.

\subsection{trainDummyClassifier.ipynb}\label{subsec:trainDummyClassifier.ipynb}

\begin{algorithm}[H]
    \caption{Training Dummy Classifiers}\label{alg:trainDummyClassifier}
    \begin{algorithmic}[1]
    \Require Preprocessed CTU13 and CICIDS2017 datasets
    \Ensure Trained Dummy Classifiers and performance metrics
    \State Import necessary libraries
    \State Read preprocessed CTU13 and CICIDS2017 datasets
    \State Define common features for training and testing
    \For{dataset in [CTU13, CICIDS2017]}
    \If{dataset is CTU13}
    \State Train Dummy Classifier on CTU13 dataset (binary classification)
    \ElsIf{dataset is CICIDS2017}
    \State Train Dummy Classifier on CICIDS2017 dataset (binary classification)
    \State Train Dummy Classifier on CICIDS2017 dataset (multiclass classification)
    \EndIf
    \EndFor
    \For{each trained Dummy Classifier}
    \State Test the classifier on its corresponding test set
    \State Calculate performance metrics (accuracy, precision, recall, F1 score)
    \State Save the trained classifier
    \EndFor
    \For{each CTU13-trained Dummy Classifier}
    \State Test the classifier on the CICIDS2017 dataset
    \State Calculate performance metrics (accuracy, precision, recall, F1 score)
    \EndFor
    \For{each CICIDS2017-trained Dummy Classifier}
    \State Test the classifier on the CTU13 dataset
    \State Calculate performance metrics (accuracy, precision, recall, F1 score)
    \EndFor
    \State Save performance metrics for analysis and comparison
    \end{algorithmic}
\end{algorithm}

This Jupyter Notebook trains and evaluates the Dummy Classifiers on the CTU13 and CICIDS2017 datasets. The classifiers' performance is a baseline for comparing the Random Forest and SVM Classifiers.

\subsection{trainRandomForest.ipynb}\label{subsec:trainRandomForest.ipynb}

This Jupyter Notebook trains and evaluates the Random Forest Classifiers on the CTU13 and CICIDS2017 datasets. It tests the classifiers' transferability by evaluating their performance on the dataset for which they were not trained. The notebook also utilizes the SHAP library to explain the classifiers' predictions and identify important features.

\subsection{trainSVM.ipynb}\label{subsec:trainSVM.ipynb}

Similar to \texttt{trainRandomForest.ipynb}~\ref{subsec:trainRandomForest.ipynb}, this Jupyter Notebook trains and evaluates the SVM Classifiers on the CTU13 and CICIDS2017 datasets, tests their transferability, and employs SHAP to interpret the classifiers' predictions and identify important features.