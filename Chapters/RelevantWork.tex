\chapter{Relevant Work}\label{chap:relevant-work}

This chapter provides a comprehensive review of existing research in machine learning-based network intrusion detection. It covers state-of-the-art approaches for botnet detection using the CTU13 dataset, intrusion detection using the CICIDS2017 dataset, and the application of Random Forest and Support Vector Machines (SVM) for intrusion detection tasks. Additionally, it explores the use of explainable AI techniques, specifically SHAP, to interpret machine learning models in the context of intrusion detection. The chapter also addresses the challenges and limitations of synthetic datasets and the dynamic nature of network traffic, emphasising the importance of considering dataset biases and representativeness. Throughout, we highlight how this dissertation’s focus on \textbf{transferability}—the ability of models trained on one dataset to perform well on another—distinguishes it from prior studies.

\section{Botnet Detection using CTU13 Dataset}

Several studies have utilised the CTU13 dataset to develop and evaluate botnet detection systems. Chowdhury et al.~\cite{chowdhury2017botnet} proposed a graph-based approach, constructing a graph representation of communication patterns among botnet-infected hosts and applying graph analysis techniques to identify botnets. They extracted features such as in-degree, out-degree, and betweenness centrality and used them to train a Random Forest classifier, achieving high detection accuracy. Their work demonstrates the potential of graph-based features for botnet detection.

Pektaş and Acarman~\cite{pektacs2019deep} applied deep learning techniques to the CTU13 dataset. They used a convolutional neural network (CNN) to learn discriminative features from raw network traffic data, converting the traffic into greyscale images for input. Their CNN-based model achieved high accuracy, showcasing the effectiveness of deep learning in capturing complex botnet patterns.

While these studies focus on developing models tailored to the CTU13 dataset, this dissertation investigates the less explored area of \textbf{transferability}. Specifically, it assesses whether models trained on the CICIDS2017 dataset can detect botnets in CTU13, contributing to understanding the adaptability of machine learning models across diverse network environments. This focus on cross-dataset performance distinguishes our work from previous CTU13-specific studies.

Having reviewed botnet detection on CTU13, we now turn to intrusion detection research using the CICIDS2017 dataset, which offers a broader range of attack types.

\section{Intrusion Detection using CICIDS2017 Dataset}

The CICIDS2017 dataset has been widely used to evaluate intrusion detection systems. Ustebay et al.~\cite{ustebay2018intrusion} proposed a multi-layer perceptron (MLP)-based system, performing extensive preprocessing and using recursive feature elimination with Random Forest for feature selection. Their approach demonstrated the potential of neural networks for intrusion detection.

Aksu and Aydin~\cite{aksu2018detecting} conducted a comparative study of machine learning algorithms on CICIDS2017, evaluating decision trees, Random Forests, and SVMs. They found that Random Forests outperformed other algorithms, highlighting the effectiveness of ensemble methods.

This dissertation leverages CICIDS2017 to train classifiers and investigate their \textbf{transferability} to detect botnet attacks in CTU13. Unlike previous studies that focus on performance within CICIDS2017, our work explores the adaptability of models to a different dataset, providing insights into cross-dataset generalisation.

The following sections focus on Random Forest and SVM, two algorithms central to this dissertation’s investigation of transferability.

\section{Random Forest for Intrusion Detection}\label{sec:RandomForestIntrusion}

Random Forest has been widely applied to intrusion detection. Farnaaz and Jabbar~\cite{farnaaz2016random} used Random Forest on the NSL-KDD dataset, employing feature selection via the Chi-square test. Their model demonstrated Random Forest’s effectiveness in detecting various network attacks.

Belouch et al.~\cite{belouch2018performance} applied Random Forest to CICIDS2017, comparing it with other algorithms like Decision Tree and Naive Bayes. Their results confirmed Random Forest’s superior performance, further validating its use in intrusion detection.

While these studies demonstrate Random Forest’s effectiveness within single datasets, this dissertation explores its \textbf{transferability} by training on CICIDS2017 and testing on CTU13. This cross-dataset approach provides new insights into the algorithm’s adaptability.

Similarly, the next section examines SVM’s role in intrusion detection and its potential for transferability.

\section{Support Vector Machines for Intrusion Detection}\label{sec:SVMIntrusion}

SVM has been extensively used for intrusion detection. Kabir et al.~\cite{kabir2017network} proposed an SVM-based system on the NSL-KDD dataset, using a genetic algorithm for feature selection and grid search for parameter optimisation. Their work highlighted SVM’s effectiveness in detecting diverse attacks.

Teng et al.~\cite{teng2017svm} applied SVM with various kernel functions to CICIDS2017, emphasising the importance of kernel selection and parameter tuning for optimal performance.

In contrast to these studies, this dissertation focuses on the \textbf{transferability} of SVM models trained on CICIDS2017 to detect botnet attacks in CTU13. By investigating cross-dataset performance, we extend existing research on SVM’s generalisation capabilities.

While Random Forest and SVM are effective for intrusion detection, understanding their decision-making processes is crucial for assessing transferability. The next section addresses this through explainable AI techniques.

\section{Explainable AI Techniques for Intrusion Detection}\label{sec:ExplainableAITechniques}

Explainable AI techniques have gained attention for providing insights into machine learning models’ decision-making. Warnecke et al.~\cite{warnecke2020evaluating} evaluated explanation methods, including SHAP, for deep learning-based intrusion detection systems. They applied SHAP to a CNN trained on NSL-KDD, demonstrating its ability to identify influential features for specific attack types.

Amarasinghe et al.~\cite{amarasinghe2018toward} used SHAP to interpret a deep neural network’s predictions on NSL-KDD, highlighting its potential for explaining complex models.

Mane and Rao~\cite{mane2021explaining} applied SHAP to a Random Forest classifier on NSL-KDD, visualising feature contributions to understand the model’s decisions.

While these studies use SHAP to interpret models within single datasets, this dissertation extends SHAP’s application to analyse \textbf{cross-dataset transferability}. By employing SHAP to interpret Random Forest and SVM classifiers trained on CICIDS2017 and tested on CTU13, we provide novel insights into why models succeed or fail in new environments. This approach enhances the interpretability of transferability, revealing dataset-specific biases and feature importance across different network settings.

Having reviewed algorithmic and explainable AI approaches, we now discuss the challenges and limitations that motivate this dissertation’s focus on transferability.

\section{Challenges and Limitations}\label{sec:ChallengesLimitations}

Despite the frequent use of CTU13 and CICIDS2017, their limitations must be acknowledged. Sommer and Paxson~\cite{sommer2010outside} critiqued synthetic datasets for failing to capture real-world network traffic’s complexity and lacking contextual information. They emphasised the need for datasets that reflect operational challenges in intrusion detection.

This dissertation addresses these limitations by investigating \textbf{transferability} across datasets, providing insights into model performance in diverse environments. Additionally, by using SHAP, we mitigate dataset biases by identifying features that are consistently important across datasets.

Moreover, Sommer and Paxson~\cite{sommer2010outside} and Buczak and Guven~\cite{buczak2015survey} highlighted the challenges posed by evolving attack patterns and dynamic network traffic. To address this, our work focuses on transferability, assessing whether models trained on one dataset can adapt to new attack types in another, contributing to more robust intrusion detection systems.

These challenges underscore the importance of this dissertation’s focus on transferability and explainable AI, which we summarise in the next section.

\section{Summary and Positioning}\label{sec:PositioningSummary}

This chapter has reviewed key studies in machine learning-based network intrusion detection, covering botnet detection (CTU13), intrusion detection (CICIDS2017), and the use of Random Forest, SVM, and SHAP. While prior research demonstrates the effectiveness of these techniques within single datasets, this dissertation investigates a novel aspect: the \textbf{transferability} of models across datasets.

Specifically, this work addresses two primary questions:
1. How effectively can Random Forest and SVM classifiers trained on CICIDS2017 detect botnet attacks in CTU13?
2. What insights can SHAP provide into the transferability of these models across datasets?

By focusing on cross-dataset performance and leveraging SHAP to interpret feature importance, this dissertation extends existing research. It provides valuable insights into the robustness and adaptability of machine learning models in diverse network environments, contributing to the development of practical intrusion detection solutions.

In summary, this chapter establishes the foundation for our investigation of transferability and explainable AI in network intrusion detection, emphasising the importance of dataset representativeness and model interpretability in addressing real-world cybersecurity challenges.