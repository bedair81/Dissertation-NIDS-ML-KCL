\chapter{Legal, Social, Ethical and Professional Issues}\label{chap:professional-issues}

The British Computer Society (BCS) Code of Conduct~\cite{bcs2021code} establishes the professional standards expected of its members, promoting excellence in competence, conduct, and ethical practice. This research is designed, executed, and disseminated in alignment with these principles.

This chapter examines the potential legal, social, ethical, and professional issues arising from the research process. It also demonstrates how the research complies with the BCS Code of Conduct.

\section{Legal Considerations}
The research utilises two publicly available datasets: CTU13~\cite{garcia2014empirical} and CICIDS2017~\cite{sharafaldin2018toward}. Both datasets are freely accessible for research purposes and are not subject to legal restrictions. No personal data is involved, as CICIDS2017 is a synthetic dataset, and CTU13 excludes passive network flows that might contain sensitive information. By avoiding personal data, the research ensures compliance with the UK Data Protection Act 2018.

\section{Ethical and Social Considerations}
This research investigates the transferability and generalisation of machine learning models across diverse datasets. As it does not involve human subjects and relies on publicly available datasets, no direct ethical or social issues arise. Nevertheless, the broader implications of developing effective network intrusion detection systems merit consideration.

By improving the detection and prevention of cyber attacks, this research enhances the security and privacy of individuals and organisations. Robust, transferable machine learning models for intrusion detection can safeguard sensitive information, prevent data breaches, and reduce risks from malicious activities in networked environments.

However, the use of explainable AI techniques, such as SHAP, introduces ethical concerns. While SHAP illuminates the decision-making processes of machine learning models, it may also reveal the most influential features for detecting specific attacks. If misused, this information could enable malicious actors to manipulate or spoof these features to evade detection. Thus, insights from SHAP must be handled with care, protected from unauthorised disclosure, and secured through robust measures to maintain their confidentiality and integrity.

Additionally, highly effective intrusion detection systems risk unintended consequences. Overreliance on automation might diminish human expertise and judgement, necessitating a balance between leveraging machine learning capabilities and preserving human oversight. False positives, which could disrupt operations and misallocate resources, also require careful management through safeguards and human review processes.

\section{Professional Considerations}
The research reveals limited transferability of machine learning models across datasets, posing challenges for organisations using these models for cybersecurity. It advises caution when deploying models across varied environments, urging organisations to enhance model generalisation to mitigate potential issues. The explainability results, however, offer insights into these limitations, laying the groundwork for future improvements.

Professionally, this research underscores the need for rigorous evaluation and validation of machine learning models in network intrusion detection. Organisations must account for dataset limitations and biases during training and testing. Techniques like SHAP highlight critical features for attack detection, fostering the development of more reliable systems.

Yet, professionals must recognise the risks of explainable AI in cybersecurity. SHAP-derived insights are sensitive and require protection from unauthorised access. Professionals bear responsibility for their ethical use, preventing inadvertent aid to malicious actors. Clear guidelines and protocols for handling these insights are essential to minimise misuse risks.

Moreover, professionals should engage in ongoing research to enhance model transferability and generalisability. Collaborative efforts within the cybersecurity community can address this dissertation’s identified limitations, advancing robust, adaptable intrusion detection solutions.

\section{Societal Impact and Sustainability}
Effective network intrusion detection systems carry profound societal implications. In an interconnected world, network security underpins public trust, protects sensitive data, and ensures critical infrastructure resilience. This research advances robust, transferable models, helping to counter cyber threats and minimise data breaches, thus fostering a sustainable digital environment.

From a sustainability perspective, improved model transferability reduces the need for extensive retraining and resource-heavy development. This adaptability supports long-term cybersecurity resilience amid evolving threats and complex networks.

However, advanced systems may engender overconfidence or complacency. Raising awareness of their limitations and advocating a multi-layered security approach is vital. False positives, which could undermine trust and cause disruptions, must be minimised through clear communication and redress mechanisms.

\section{British Computing Society Code of Conduct}
Conducted with integrity and professionalism, this research adheres to the BCS Code of Conduct. Results are presented transparently using unrestricted, publicly available datasets, and no direct ethical or social issues arise. Findings are communicated clearly, with limitations acknowledged.

The research aligns with BCS principles by promoting responsible technology use and advancing cybersecurity knowledge. It upholds honesty, integrity, and objectivity while addressing network intrusion detection—a key public interest issue—through effective, reliable solutions.

It also reflects professional competence and a commitment to continuous improvement, building on existing knowledge to enhance understanding of model transferability and explainability. Insights from this work guide future research and professional development in cybersecurity.

Nonetheless, the research acknowledges risks tied to explainable AI, particularly with SHAP. In line with the BCS Code of Conduct, it stresses responsible handling of these insights, advocating guidelines to prevent misuse. By addressing these ethical considerations, the research exemplifies BCS-aligned responsible AI use in cybersecurity.