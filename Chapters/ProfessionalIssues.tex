\chapter{Legal, Social, Ethical and Professional Issues}
This chapter evaluates potential legal, social, ethical, and professional issues that may arise during the research process. It concludes by explaining how the research adheres to the British Computing Society's Code of Conduct.

\section{Legal Considerations}
The research utilises two publicly available datasets: CTU13\cite{garcia2014empirical} and CICIDS2017\cite{sharafaldin2018toward}. These datasets are accessible for research purposes and are not subject to legal restrictions. The research does not involve the use of personal data, as CICIDS2017 is a synthetic dataset and CTU13 has excluded passive network flows that could potentially contain sensitive information. By avoiding the use of personal data, the research ensures compliance with the UK Data Protection Act 2018.

\section{Ethical \& Social Considerations}
The focus of this research is on evaluating the transferability and generalisation of machine learning models across different datasets. The study does not involve human subjects, and the datasets used are publicly available. Consequently, there are no direct ethical or social issues associated with this research. However, it is important to consider the broader implications of developing effective network intrusion detection systems. By enhancing the ability to detect and prevent cyber attacks, this research contributes to improving the overall security and privacy of individuals and organisations. The development of robust and transferable machine learning models for intrusion detection can help protect sensitive information, prevent data breaches, and mitigate the risks associated with malicious activities in networked environments.

On the other hand, the use of explainable AI techniques, such as SHAP, in the context of network intrusion detection raises some ethical concerns. While SHAP provides valuable insights into the decision-making process of machine learning models, it can also potentially expose the most influential features for detecting specific types of attacks. This information, if fallen into the wrong hands, could be exploited by malicious actors to evade detection by manipulating or spoofing the relevant features. Therefore, it is crucial to ensure that the insights gained from SHAP are handled with utmost care and are not disclosed to unauthorized parties. Proper security measures should be in place to protect the confidentiality and integrity of the explainability results.

Furthermore, the development of highly effective intrusion detection systems may also have unintended consequences. For instance, it could lead to an overreliance on automated systems for security, potentially diminishing the role of human expertise and judgment. It is important to strike a balance between leveraging the capabilities of machine learning models and maintaining human oversight and intervention in the decision-making process. Additionally, the potential for false positives in intrusion detection systems should be carefully considered, as it can result in unnecessary disruptions and misallocation of resources. Adequate safeguards and human review processes should be in place to mitigate the impact of false positives.

\section{Professional Considerations}
The research findings indicate that the current state of machine learning model transferability across different datasets is limited, which can be problematic for organisations relying on these models for cybersecurity purposes. The study suggests that organisations should exercise caution when deploying machine learning models across different environments, as the models may not generalise well. However, the explainability results provide insights into the reasons behind this limitation, paving the way for future work aimed at improving the transferability of machine learning models.

From a professional perspective, this research highlights the importance of thorough evaluation and validation of machine learning models in the context of network intrusion detection. It emphasizes the need for organisations to carefully consider the limitations and potential biases of the datasets used for training and testing these models. The findings underscore the significance of explainable AI techniques, such as SHAP, in providing insights into the decision-making process of machine learning models. These insights can aid in identifying the most relevant features for detecting specific types of attacks and guide the development of more robust and reliable intrusion detection systems.

However, professionals should also be aware of the potential risks associated with the use of explainable AI techniques in cybersecurity. The insights gained from techniques like SHAP should be treated as sensitive information and should be protected from unauthorized access. Professionals have a responsibility to ensure that the explainability results are used ethically and responsibly, and that they do not inadvertently aid malicious actors in evading detection. It is crucial to establish clear guidelines and protocols for handling and sharing the insights derived from explainable AI techniques to minimize the risk of misuse.

Furthermore, professionals should actively engage in ongoing research and development efforts to improve the transferability and generalisability of machine learning models in the context of network intrusion detection. Collaborative efforts within the cybersecurity community can help address the limitations identified in this study and contribute to the development of more robust and adaptable intrusion detection solutions.

\section{Societal Impact and Sustainability}
The development of effective network intrusion detection systems has significant societal implications. In an increasingly interconnected world, the security and integrity of computer networks are crucial for maintaining public trust, protecting sensitive information, and ensuring the smooth functioning of critical infrastructure. This research contributes to the development of more robust and transferable machine learning models for intrusion detection, which can help safeguard against cyber attacks and minimize the potential for data breaches. By enhancing the security of networked systems, this research promotes a more sustainable and resilient digital environment.

From a sustainability perspective, the research findings can guide the development of intrusion detection solutions that are more adaptable and scalable. By improving the transferability of machine learning models across different datasets and network environments, this research supports the long-term sustainability of cybersecurity measures. It enables organizations to leverage existing knowledge and models to detect and respond to emerging threats, reducing the need for extensive retraining and resource-intensive model development processes. This sustainability aspect is particularly important in the face of constantly evolving cyber attack landscapes and the increasing complexity of networked systems.

However, it is important to acknowledge that the development of advanced intrusion detection systems may also have unintended consequences for society. The increasing reliance on automated systems for cybersecurity could potentially lead to a false sense of security and complacency. It is crucial to raise awareness among individuals and organizations about the limitations of these systems and the importance of maintaining vigilance and adopting a multi-layered approach to security. Additionally, the societal impact of false positives generated by intrusion detection systems should be carefully considered, as they can lead to unnecessary disruptions and erode trust in these systems. Efforts should be made to minimize false positives and provide clear communication and redress mechanisms to mitigate their impact on individuals and organizations.

\section{British Computing Society Code of Conduct}
This research is conducted in accordance with the British Computing Society's Code of Conduct. The study is carried out with integrity and professionalism, and the results are presented accurately and transparently. The research does not involve any direct ethical or social issues, and the datasets used are publicly available and free from legal restrictions. The findings of the research are presented in a clear and understandable manner, and the limitations of the study are acknowledged.

The research aligns with the principles of the BCS Code of Conduct by promoting the responsible use of technology and contributing to the advancement of knowledge in the field of cybersecurity. The study adheres to the principles of honesty, integrity, and objectivity in the conduct of research and the dissemination of findings. The research also demonstrates a commitment to the public interest by addressing the critical issue of network intrusion detection and working towards the development of more effective and reliable security solutions.

Furthermore, the research acknowledges the importance of professional competence and the need for continuous learning and improvement. The study builds upon existing knowledge in the field and seeks to advance the understanding of machine learning model transferability and explainability in the context of intrusion detection. The research findings provide valuable insights that can inform future research directions and contribute to the ongoing development of cybersecurity professionals.

However, the research also recognizes the potential risks and ethical considerations associated with the use of explainable AI techniques in cybersecurity. In adherence to the BCS Code of Conduct, the research emphasizes the need for responsible handling and protection of the insights gained from techniques like SHAP.\@ It highlights the importance of establishing clear guidelines and protocols to prevent the misuse of explainability results by malicious actors. By addressing these ethical considerations and promoting the responsible use of AI in cybersecurity, the research demonstrates alignment with the principles of the BCS Code of Conduct.